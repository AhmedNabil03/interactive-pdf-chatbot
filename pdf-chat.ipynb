{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbf003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain langchain_community langchain-huggingface\n",
    "# !pip install -qU sentence-transformers faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36138a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17211a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_vectordb(pdf_file):\n",
    "    loader = PyPDFLoader(pdf_file)\n",
    "    documents = loader.load_and_split()\n",
    "    texts = [chunk.page_content for chunk in documents]\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vec_db = FAISS.from_texts(texts, embeddings)\n",
    "    return vec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b2fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_retriever(vector_db):\n",
    "    return vector_db.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8eb0a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_model(model_name, api_key, temperature):\n",
    "    return HuggingFaceHub(\n",
    "        repo_id=model_name,\n",
    "        model_kwargs={\"temperature\": temperature, \"max_tokens\": 512},\n",
    "        huggingfacehub_api_token=api_key,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c6aa8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant who provides concise and accurate answers based on the given context.\"\n",
    ")\n",
    "human_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"Given the following context:\\n{context}\\n\\nAnswer the question:\\n{question}\"\n",
    ")\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f54a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_chain(model_name, api_key, temperature, chat_prompt, pdf_path):\n",
    "    llm = llm_model(model_name, api_key, temperature)\n",
    "\n",
    "    vec_store = extract_and_vectordb(pdf_path)\n",
    "    retriever = vec_retriever(vec_store)\n",
    "\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        chain_type_kwargs={\"prompt\": chat_prompt}\n",
    "        )\n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4339c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(qa_chain, query):\n",
    "    response = qa_chain.invoke({\"query\": query})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4da602a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(response, query):\n",
    "    result = response['result']\n",
    "    query_index = result.rfind(query)\n",
    "    if query_index != -1:\n",
    "        answer = result[query_index + len(query):].strip()\n",
    "        return answer\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACEHUB_API_TOKEN = \"\"\n",
    "api_key = HUGGINGFACEHUB_API_TOKEN\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "pdf_path = \"ai_doc.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94027bdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed_3hijq3m\\AppData\\Local\\Temp\\ipykernel_9100\\2043276256.py:2: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  return HuggingFaceHub(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ahmed_3hijq3m\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qa_chain = initialize_chain(\n",
    "    model_name=model_name, \n",
    "    api_key=api_key, \n",
    "    temperature=0.1, \n",
    "    chat_prompt=chat_prompt, \n",
    "    pdf_path=pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96750642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main topic of the PDF is the impact of Artificial Intelligence (AI) on Data Analytics.\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the main topic of the pdf?\"\n",
    "answer = get_answer(\n",
    "    qa_chain,\n",
    "    query)\n",
    "print(postprocess(answer, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f88427d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sections in the provided PDF are:\n",
      "1. Title\n",
      "2. Abstract\n",
      "3. Introduction\n",
      "4. Body (with subsections)\n",
      "   a. Machine Learning in Data Analytics\n",
      "   b. Deep Learning and Neural Networks\n",
      "   c. AI-Driven Predictive Analytics\n",
      "   d. Automation of Data Processing\n",
      "5. Conclusion\n",
      "6. Summary\n"
     ]
    }
   ],
   "source": [
    "query = \"what are the pdf sections?\"\n",
    "answer = get_answer(\n",
    "    qa_chain,\n",
    "    query)\n",
    "print(postprocess(answer, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0837829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence of the Summary is: \"This paper discussed the transformative role of AI in data analytics.\"\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the first sentence of the Summary?\"\n",
    "answer = get_answer(\n",
    "    qa_chain,\n",
    "    query)\n",
    "print(postprocess(answer, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a105c94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
